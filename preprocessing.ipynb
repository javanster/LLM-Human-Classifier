{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "This notebook contains:\n",
    "- Code to clean the chosen datasets\n",
    "- Code to extract and engineer features:\n",
    "    - TF-IDF of the tokens with the highest TF-IDF values in the training dataset\n",
    "    - Flesch reading ease of the text\n",
    "    - Zipfient coefficient\n",
    "    - Vocabulary diversity\n",
    "    - Average sentence length\n",
    "    - Dot frequency\n",
    "    - Comma frequency\n",
    "    - Question mark frequency\n",
    "    - Exclamation frequency\n",
    "    - Stopword frequency\n",
    "    - Noun frequency, normalized\n",
    "    - Determiner frequency, normalized\n",
    "    - Conjunction frequency, normalized\n",
    "    - Auxiliary verbs, normalized\n",
    "    - Positive sentiment score\n",
    "    - Negative sentiment score\n",
    "    - Neutral sentiment score\n",
    "    - Compound sentiment score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size before removing rows identical to the training data: 73573\n",
      "Train and pretest data size: 158294\n",
      "Validation data size: 1679\n",
      "Test data size: 40202\n",
      "The valdation data is 0.84% of the total data\n"
     ]
    }
   ],
   "source": [
    "# Load datasets, remove duplicates and check the size of the datasets\n",
    "import pandas as pd\n",
    "\n",
    "train_and_pretest_data = pd.read_csv('data/detect_ai.csv')\n",
    "valid_data = pd.read_csv('data/detect_ai_validation.csv')\n",
    "test_data = pd.read_csv('data/daigt_v4.csv')\n",
    "\n",
    "print(f\"Test data size before removing rows identical to the training data: {len(test_data)}\")\n",
    "\n",
    "train_and_pretest_data = train_and_pretest_data.drop_duplicates(subset='text')\n",
    "test_data = test_data[~test_data['text'].isin(train_and_pretest_data['text'])]\n",
    "valid_data = valid_data[~valid_data['text'].isin(train_and_pretest_data['text'])]\n",
    "\n",
    "print(f\"Train and pretest data size: {len(train_and_pretest_data)}\")\n",
    "print(f\"Validation data size: {len(valid_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n",
    "\n",
    "valid_data_percentage = len(valid_data) / (len(train_and_pretest_data) + len(test_data) + len(valid_data))\n",
    "\n",
    "print(f\"The valdation data is {valid_data_percentage * 100:.2f}% of the total data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comment on valid_data:\n",
    "This was first a dataset that was going to be used as validation set for the nn_model. But after making the decision to not use it as validation set we kept in in this preprocessing jupyter file because of it's small size to be able to quickly check that the preprocessing is working without having to process for 3 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change this variable to True if you want to process all datasets\n",
    "Expected runtime if True: 2-3h\n",
    "\n",
    "Expected runtime if False: 2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "runAll = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and pretest data columns: Index(['id', 'prompt_id', 'text', 'generated'], dtype='object')\n",
      "\n",
      "Validation data columns: Index(['id', 'prompt_id', 'text', 'generated'], dtype='object')\n",
      "\n",
      "Test data columns: Index(['text', 'label', 'prompt_name', 'source', 'RDizzl3_seven', 'model'], dtype='object')\n",
      "\n",
      "Test data[model] has the following values: ['human' 'mistral' 'llama' 'gpt' 'claude' 'falcon' 'palm' 'cohere' 'ada'\n",
      " 'babbage' 'curie' 'davinci']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train and pretest data columns: {train_and_pretest_data.columns}\\n\")\n",
    "print(f\"Validation data columns: {valid_data.columns}\\n\")\n",
    "print(f\"Test data columns: {test_data.columns}\\n\")\n",
    "print(f\"Test data[model] has the following values: {test_data['model'].unique()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and pretest data columns: Index(['text', 'generated'], dtype='object')\n",
      "Validation data columns: Index(['text', 'generated'], dtype='object')\n",
      "Test data columns: Index(['text', 'generated'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In recent years, there has been a growing move...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---\\nWhy not cars in our life\\n===============...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A car is considered by many a nessecity for ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H\\n\\nello fellow citezens , we are here to inf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have you ever known how if feels not being abl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  generated\n",
       "0  In recent years, there has been a growing move...          1\n",
       "1  ---\\nWhy not cars in our life\\n===============...          1\n",
       "2  A car is considered by many a nessecity for ev...          1\n",
       "3  H\\n\\nello fellow citezens , we are here to inf...          0\n",
       "4  Have you ever known how if feels not being abl...          1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Irrelevant Data\n",
    "# and renaming the columns to be the same for all datasets\n",
    "# to ['text', 'generated'] as string, boolean for all datasets\n",
    "train_and_pretest_data = train_and_pretest_data[['text', 'generated']]\n",
    "valid_data = valid_data[['text', 'generated']]\n",
    "\n",
    "test_data['generated'] = (test_data['model'] != 'human').astype(int)\n",
    "test_data = test_data[['text', 'generated']]\n",
    "\n",
    "print(f\"Train and pretest data columns: {train_and_pretest_data.columns}\")\n",
    "print(f\"Validation data columns: {valid_data.columns}\")\n",
    "print(f\"Test data columns: {test_data.columns}\")\n",
    "\n",
    "train_and_pretest_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and pretest data into train and pretest using stratify,\n",
    "# keeping the same distribution of generated values in the new datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, pretest_data = train_test_split(train_and_pretest_data, test_size=0.2, stratify=train_and_pretest_data['generated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport nltk\\nnltk.download('punkt')\\nnltk.download('wordnet')\\nnltk.download('stopwords')\\nnltk.download('averaged_perceptron_tagger')\\nnltk.download('vader_lexicon')\\n\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download these if it's your first time running the code\n",
    "\"\"\"\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from nltk import word_tokenize\n",
    "\n",
    "valid_data['tokenized_text'] = valid_data['text'].apply(word_tokenize)\n",
    "if runAll:\n",
    "    train_data['tokenized_text'] = train_data['text'].apply(word_tokenize)\n",
    "    pretest_data['tokenized_text'] = pretest_data['text'].apply(word_tokenize)\n",
    "    test_data['tokenized_text'] = test_data['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "\n",
    "# Use of Co-Pilot:\n",
    "# Here and in cells below, GitHub Co-pilot has been used to repeat the \"runAll\" pattern\n",
    "# and use the same function as our code for the other 3 datasets\n",
    "\n",
    "valid_data['tokenized_text'] = valid_data['tokenized_text'].apply(lambda x: [word.lower() for word in x])\n",
    "if runAll:\n",
    "    train_data['tokenized_text'] = train_data['tokenized_text'].apply(lambda x: [word.lower() for word in x])\n",
    "    pretest_data['tokenized_text'] = pretest_data['tokenized_text'].apply(lambda x: [word.lower() for word in x])\n",
    "    test_data['tokenized_text'] = test_data['tokenized_text'].apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>noun_freq</th>\n",
       "      <th>determiner_freq</th>\n",
       "      <th>conjunction_freq</th>\n",
       "      <th>auxiliary_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_,_ _and it has to do with the fact that if yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[_, ,, _, _and, it, has, to, do, with, the, fa...</td>\n",
       "      <td>[(_, NN), (,, ,), (_, FW), (_and, NN), (it, PR...</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.082524</td>\n",
       "      <td>0.150485</td>\n",
       "      <td>0.228155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There are advantages of limiting car usage les...</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, are, advantages, of, limiting, car, us...</td>\n",
       "      <td>[(there, EX), (are, VBP), (advantages, NNS), (...</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Limiting car usage ii very beneifial to not on...</td>\n",
       "      <td>1</td>\n",
       "      <td>[limiting, car, usage, ii, very, beneifial, to...</td>\n",
       "      <td>[(limiting, VBG), (car, NN), (usage, NN), (ii,...</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0.057823</td>\n",
       "      <td>0.125850</td>\n",
       "      <td>0.200680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cars have been one of the most advanced invent...</td>\n",
       "      <td>1</td>\n",
       "      <td>[cars, have, been, one, of, the, most, advance...</td>\n",
       "      <td>[(cars, NNS), (have, VBP), (been, VBN), (one, ...</td>\n",
       "      <td>0.169082</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.140097</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are cars even really necessary? Vehicles can c...</td>\n",
       "      <td>1</td>\n",
       "      <td>[are, cars, even, really, necessary, ?, vehicl...</td>\n",
       "      <td>[(are, VBP), (cars, NNS), (even, RB), (really,...</td>\n",
       "      <td>0.397866</td>\n",
       "      <td>0.099085</td>\n",
       "      <td>0.099085</td>\n",
       "      <td>0.102134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  generated  \\\n",
       "0  _,_ _and it has to do with the fact that if yo...          1   \n",
       "1  There are advantages of limiting car usage les...          1   \n",
       "2  Limiting car usage ii very beneifial to not on...          1   \n",
       "3  Cars have been one of the most advanced invent...          1   \n",
       "4  Are cars even really necessary? Vehicles can c...          1   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [_, ,, _, _and, it, has, to, do, with, the, fa...   \n",
       "1  [there, are, advantages, of, limiting, car, us...   \n",
       "2  [limiting, car, usage, ii, very, beneifial, to...   \n",
       "3  [cars, have, been, one, of, the, most, advance...   \n",
       "4  [are, cars, even, really, necessary, ?, vehicl...   \n",
       "\n",
       "                                            pos_tags  noun_freq  \\\n",
       "0  [(_, NN), (,, ,), (_, FW), (_and, NN), (it, PR...   0.194175   \n",
       "1  [(there, EX), (are, VBP), (advantages, NNS), (...   0.196000   \n",
       "2  [(limiting, VBG), (car, NN), (usage, NN), (ii,...   0.210884   \n",
       "3  [(cars, NNS), (have, VBP), (been, VBN), (one, ...   0.169082   \n",
       "4  [(are, VBP), (cars, NNS), (even, RB), (really,...   0.397866   \n",
       "\n",
       "   determiner_freq  conjunction_freq  auxiliary_freq  \n",
       "0         0.082524          0.150485        0.228155  \n",
       "1         0.056000          0.128000        0.228000  \n",
       "2         0.057823          0.125850        0.200680  \n",
       "3         0.072464          0.140097        0.217391  \n",
       "4         0.099085          0.099085        0.102134  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part-of-Speech Tagging\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "valid_data['pos_tags'] = valid_data['tokenized_text'].apply(pos_tag)\n",
    "\n",
    "if runAll:\n",
    "    train_data['pos_tags'] = train_data['tokenized_text'].apply(pos_tag)\n",
    "    pretest_data['pos_tags'] = pretest_data['tokenized_text'].apply(pos_tag)\n",
    "    test_data['pos_tags'] = test_data['tokenized_text'].apply(pos_tag)\n",
    "\n",
    "# This is a function that counts specific POS tags and normalize them by the text length\n",
    "def add_normalized_pos_features(df):\n",
    "    def normalize_pos_counts(text_tokens):\n",
    "        total_words = len(text_tokens)\n",
    "        if total_words == 0:\n",
    "            return {'noun_freq': 0, 'determiner_freq': 0, 'conjunction_freq': 0, 'auxiliary_freq': 0}\n",
    "        \n",
    "        tags = pos_tag(text_tokens)\n",
    "        tag_counts = Counter(tag for word, tag in tags)\n",
    "        noun_freq = sum(tag_counts[tag] for tag in ['NN', 'NNS', 'NNP', 'NNPS']) / total_words # ('NN' for singular common nouns, 'NNS' for plural common nouns, 'NNP' for singular proper nouns, 'NNPS' for plural proper nouns)\n",
    "        determiner_freq = tag_counts['DT'] / total_words\n",
    "        conjunction_freq = sum(tag_counts[tag] for tag in ['CC', 'IN']) / total_words #  ('CC' for coordinating conjunctions, 'IN' for subordinating or prepositions)\n",
    "        auxiliary_freq = sum(tag_counts[tag] for tag in ['MD', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']) / total_words # ('MD' for modals, 'VB' for base form, 'VBD' for past tense, 'VBG' for gerund or present participle, 'VBN' for past participle, 'VBP' for non-3rd person singular present, 'VBZ' for 3rd person singular present)\n",
    "\n",
    "        return {\n",
    "            'noun_freq': noun_freq,\n",
    "            'determiner_freq': determiner_freq,\n",
    "            'conjunction_freq': conjunction_freq,\n",
    "            'auxiliary_freq': auxiliary_freq\n",
    "        }\n",
    "\n",
    "    pos_features = df['tokenized_text'].apply(normalize_pos_counts)\n",
    "    pos_features_df = pos_features.apply(pd.Series)\n",
    "    for column in pos_features_df.columns:\n",
    "        df[column] = pos_features_df[column]\n",
    "\n",
    "add_normalized_pos_features(valid_data)\n",
    "if runAll:\n",
    "    add_normalized_pos_features(train_data)\n",
    "    add_normalized_pos_features(pretest_data)\n",
    "    add_normalized_pos_features(test_data)\n",
    "\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>av_sentence_length</th>\n",
       "      <th>vocabulary_diversity</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>zipfian_coefficient</th>\n",
       "      <th>dot_freq</th>\n",
       "      <th>comma_freq</th>\n",
       "      <th>question_freq</th>\n",
       "      <th>exclamation_freq</th>\n",
       "      <th>stopword_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.666667</td>\n",
       "      <td>0.577670</td>\n",
       "      <td>77.77</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.666667</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>57.27</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>68.81</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>83.80</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.939394</td>\n",
       "      <td>0.027439</td>\n",
       "      <td>96.18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   av_sentence_length  vocabulary_diversity  flesch_reading_ease  \\\n",
       "0           68.666667              0.577670                77.77   \n",
       "1           41.666667              0.432000                57.27   \n",
       "2           21.000000              0.510204                68.81   \n",
       "3           23.000000              0.536232                83.80   \n",
       "4            9.939394              0.027439                96.18   \n",
       "\n",
       "   zipfian_coefficient  dot_freq  comma_freq  question_freq  exclamation_freq  \\\n",
       "0             1.125000         8           8              2                 0   \n",
       "1             1.300000         6           6              0                 0   \n",
       "2             1.142857        14          10              0                 0   \n",
       "3             1.222222         9           3              0                 0   \n",
       "4             1.000000        64           0              1                 0   \n",
       "\n",
       "   stopword_freq  \n",
       "0            100  \n",
       "1            118  \n",
       "2            139  \n",
       "3            111  \n",
       "4            262  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Complexity and Diversity\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Average sentence length\n",
    "def average_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if sentences:  # Check if there are any sentences to avoid division by zero\n",
    "        return sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "    else:\n",
    "        return 0  # Return 0 if text contains no sentences\n",
    "\n",
    "valid_data['av_sentence_length'] = valid_data['text'].apply(average_sentence_length)\n",
    "\n",
    "if runAll:\n",
    "    train_data['av_sentence_length'] = train_data['text'].apply(average_sentence_length)\n",
    "    pretest_data['av_sentence_length'] = pretest_data['text'].apply(average_sentence_length)\n",
    "    test_data['av_sentence_length'] = test_data['text'].apply(average_sentence_length)\n",
    "\n",
    "\n",
    "# Vocabulary diversity\n",
    "\"\"\"The ratio of unique words to the total number of words in a text\"\"\"\n",
    "from nltk import FreqDist\n",
    "\n",
    "valid_data['vocabulary_diversity'] = valid_data['tokenized_text'].apply(lambda x: len(set(x)) / len(x))\n",
    "if runAll:\n",
    "    train_data['vocabulary_diversity'] = train_data['tokenized_text'].apply(lambda x: len(set(x)) / len(x))\n",
    "    pretest_data['vocabulary_diversity'] = pretest_data['tokenized_text'].apply(lambda x: len(set(x)) / len(x))\n",
    "    test_data['vocabulary_diversity'] = test_data['tokenized_text'].apply(lambda x: len(set(x)) / len(x))\n",
    "\n",
    "\n",
    "# Readability\n",
    "# This explaination is written by ChatGPT\n",
    "\"\"\"The Flesch Reading Ease score is a\n",
    "readability test that provides a numerical\n",
    "score indicating how easy or difficult a\n",
    "text is to understand. The score is calculated\n",
    "based on the average length of sentences and\n",
    "the average number of syllables per word in the text.\n",
    "Scores typically range from 0 to 100,\n",
    "with higher scores indicating easier readability.\"\"\"\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "valid_data['flesch_reading_ease'] = valid_data['text'].apply(flesch_reading_ease)\n",
    "if runAll:\n",
    "    train_data['flesch_reading_ease'] = train_data['text'].apply(flesch_reading_ease)\n",
    "    pretest_data['flesch_reading_ease'] = pretest_data['text'].apply(flesch_reading_ease)\n",
    "    test_data['flesch_reading_ease'] = test_data['text'].apply(flesch_reading_ease)\n",
    "\n",
    "\n",
    "# Zipfian Coefficient\n",
    "# This explaination and function is written by ChatGPT\n",
    "\"\"\"The Zipfian coefficient is a measure of the\n",
    "distribution of word frequencies in a text.\n",
    "It is calculated by plotting the frequency of\n",
    "each word in the text against its rank in the\n",
    "frequency table and fitting a curve to the data.\n",
    "\"\"\"\n",
    "from nltk import ngrams\n",
    "\n",
    "def zipfian_coefficient(text, n=1):\n",
    "    ngrams_list = list(ngrams(text, n))\n",
    "    freq_dist = FreqDist(ngrams_list)\n",
    "    freq_values = list(freq_dist.values())\n",
    "    if len(freq_values) >= 2:\n",
    "        freq_values.sort(reverse=True)\n",
    "        return freq_values[0] / freq_values[1]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "valid_data['zipfian_coefficient'] = valid_data['tokenized_text'].apply(zipfian_coefficient)\n",
    "if runAll:\n",
    "    train_data['zipfian_coefficient'] = train_data['tokenized_text'].apply(zipfian_coefficient)\n",
    "    pretest_data['zipfian_coefficient'] = pretest_data['tokenized_text'].apply(zipfian_coefficient)\n",
    "    test_data['zipfian_coefficient'] = test_data['tokenized_text'].apply(zipfian_coefficient)\n",
    "\n",
    "\n",
    "# Punctuation usage\n",
    "# (They will later be normalized in the normalization part)\n",
    "def add_punctuation_features(df):\n",
    "    df['dot_freq'] = df['text'].str.count('\\\\.')\n",
    "    df['comma_freq'] = df['text'].str.count(',')\n",
    "    df['question_freq'] = df['text'].str.count('\\\\?')\n",
    "    df['exclamation_freq'] = df['text'].str.count('!')\n",
    "\n",
    "add_punctuation_features(valid_data)\n",
    "if runAll:\n",
    "    add_punctuation_features(train_data)\n",
    "    add_punctuation_features(pretest_data)\n",
    "    add_punctuation_features(test_data)\n",
    "\n",
    "\n",
    "# Stopword Frequency\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def add_stopword_count_feature(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['stopword_freq'] = df['tokenized_text'].apply(lambda x: sum(1 for word in x if word in stop_words))\n",
    "\n",
    "add_stopword_count_feature(valid_data)\n",
    "if runAll:\n",
    "    add_stopword_count_feature(train_data)\n",
    "    add_stopword_count_feature(pretest_data)\n",
    "    add_stopword_count_feature(test_data)\n",
    "\n",
    "# New features added:\n",
    "valid_data[['av_sentence_length', 'vocabulary_diversity', 'flesch_reading_ease', 'zipfian_coefficient', 'dot_freq', 'comma_freq', 'question_freq', 'exclamation_freq', 'stopword_freq']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_data['tokenized_text'] after removing stopwords: 0    [_, ,, _, _and, fact, automobile, ,, danger, g...\n",
      "1    [advantages, limiting, car, usage, less, green...\n",
      "2    [limiting, car, usage, ii, beneifial, envirome...\n",
      "3    [cars, one, advanced, invention, world, ;, las...\n",
      "4    [cars, even, really, necessary, ?, vehicles, c...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "valid_data['tokenized_text'] = valid_data['tokenized_text'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
    "if runAll:\n",
    "    train_data['tokenized_text'] = train_data['tokenized_text'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
    "    pretest_data['tokenized_text'] = pretest_data['tokenized_text'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
    "    test_data['tokenized_text'] = test_data['tokenized_text'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
    "\n",
    "print(f\"valid_data['tokenized_text'] after removing stopwords: {valid_data['tokenized_text'].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of non-alphabetic characters\n",
    "\n",
    "valid_data['tokenized_text'] = valid_data['tokenized_text'].apply(lambda x: [word for word in x if word.isalpha()])\n",
    "if runAll:\n",
    "    train_data['tokenized_text'] = train_data['tokenized_text'].apply(lambda x: [word for word in x if word.isalpha()])\n",
    "    pretest_data['tokenized_text'] = pretest_data['tokenized_text'].apply(lambda x: [word for word in x if word.isalpha()])    \n",
    "    test_data['tokenized_text'] = test_data['tokenized_text'].apply(lambda x: [word for word in x if word.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_data['tokenized_text'] after lemmatization: 0    [fact, automobile, danger, getting, author, co...\n",
      "1    [advantage, limiting, car, usage, le, greenhou...\n",
      "2    [limiting, car, usage, ii, beneifial, envirome...\n",
      "3    [car, one, advanced, invention, world, lasted,...\n",
      "4    [car, even, really, necessary, vehicle, cause,...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "valid_data['tokenized_text'] = valid_data['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "if runAll:\n",
    "    train_data['tokenized_text'] = train_data['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    pretest_data['tokenized_text'] = pretest_data['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    test_data['tokenized_text'] = test_data['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "print(f\"valid_data['tokenized_text'] after lemmatization: {valid_data['tokenized_text'].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.8277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.9392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.5878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.9996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound\n",
       "0  0.069  0.878  0.053    0.1397\n",
       "1  0.069  0.832  0.099    0.8277\n",
       "2  0.044  0.868  0.087    0.9392\n",
       "3  0.088  0.802  0.110    0.5878\n",
       "4  0.311  0.689  0.000   -0.9996"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "valid_data['sentiment_scores'] = valid_data['text'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x))\n",
    "sentiment_columns = valid_data['sentiment_scores'].apply(pd.Series)\n",
    "valid_data = pd.concat([valid_data, sentiment_columns], axis=1)\n",
    "valid_data.drop('sentiment_scores', axis=1, inplace=True)\n",
    "\n",
    "if runAll:\n",
    "    train_data['sentiment_scores'] = train_data['text'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x))\n",
    "    sentiment_columns = train_data['sentiment_scores'].apply(pd.Series)\n",
    "    train_data = pd.concat([train_data, sentiment_columns], axis=1)\n",
    "    train_data.drop('sentiment_scores', axis=1, inplace=True)\n",
    "\n",
    "    pretest_data['sentiment_scores'] = pretest_data['text'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x))\n",
    "    sentiment_columns = pretest_data['sentiment_scores'].apply(pd.Series)\n",
    "    pretest_data = pd.concat([pretest_data, sentiment_columns], axis=1)\n",
    "    pretest_data.drop('sentiment_scores', axis=1, inplace=True)\n",
    "\n",
    "    test_data['sentiment_scores'] = test_data['text'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x))\n",
    "    sentiment_columns = test_data['sentiment_scores'].apply(pd.Series)\n",
    "    test_data = pd.concat([test_data, sentiment_columns], axis=1)\n",
    "    test_data.drop('sentiment_scores', axis=1, inplace=True)\n",
    "\n",
    "valid_data[['neg', 'neu', 'pos', 'compound']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ability', 'able', 'aboard', 'abolish', 'abolished', 'abolishing',\n",
       "       'absence', 'absolute', 'absolutely', 'abuse', 'abused', 'academic',\n",
       "       'academically', 'accelerate', 'accelerating', 'accept',\n",
       "       'acceptable', 'acceptance', 'accepted', 'access', 'accessibility',\n",
       "       'accessible', 'accident', 'acciedent', 'accommodate', 'accomplish',\n",
       "       'accomplished', 'accomplishment', 'according', 'accordingly',\n",
       "       'account', 'accountability', 'accountable', 'accuracy', 'accurate',\n",
       "       'accurately', 'accusation', 'accustomed', 'ace', 'achievable',\n",
       "       'achieve', 'achieved', 'achievement', 'achieving', 'acid',\n",
       "       'acknowledge', 'acknowledged', 'acknowledges', 'acknowledging',\n",
       "       'acquire', 'acquired', 'acquiring', 'acquisition', 'acropolis',\n",
       "       'across', 'act', 'acting', 'action', 'active', 'actively',\n",
       "       'activites', 'activity', 'actor', 'actual', 'actually', 'ad',\n",
       "       'adam', 'adapt', 'adaptation', 'adapting', 'add', 'added',\n",
       "       'addicted', 'addictive', 'adding', 'addition', 'additional',\n",
       "       'additionally', 'address', 'addressed', 'addressing', 'adequate',\n",
       "       'adequately', 'adhere', 'adherence', 'adjust', 'adjusted',\n",
       "       'adjusting', 'adjustment', 'administration', 'administrator',\n",
       "       'admission', 'admit', 'admitted', 'adolescent', 'adopt', 'adopted',\n",
       "       'adopting', 'adoption', 'adult'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Limit to 5000 features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "if not runAll:\n",
    "    valid_data['text_for_tfidf'] = valid_data['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "    valid_data_sparse = vectorizer.fit_transform(valid_data['text_for_tfidf'])\n",
    "\n",
    "if runAll: # then use the same 5000 tokens for the 3 other datasets, based on train_data\n",
    "    train_data['text_for_tfidf'] = train_data['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "    vectorizer.fit(train_data['text_for_tfidf'])\n",
    "    train_data_sparse = vectorizer.transform(train_data['text_for_tfidf'])\n",
    "\n",
    "    pretest_data['text_for_tfidf'] = pretest_data['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "    pretest_data_sparse = vectorizer.transform(pretest_data['text_for_tfidf'])\n",
    "\n",
    "    valid_data['text_for_tfidf'] = valid_data['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "    valid_data_sparse = vectorizer.transform(valid_data['text_for_tfidf'])\n",
    "\n",
    "    test_data['text_for_tfidf'] = test_data['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "    test_data_sparse = vectorizer.transform(test_data['text_for_tfidf'])\n",
    "\n",
    "valid_data_sparse.shape\n",
    "sparse_matrices_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# The first 100 feature names\n",
    "sparse_matrices_feature_names[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all text columns and splitting into x and y\n",
    "# the datasets are purely numeric after this\n",
    "\n",
    "valid_data_y = valid_data['generated']\n",
    "valid_data.drop(columns=['generated', 'text', 'tokenized_text', 'pos_tags', 'text_for_tfidf'], inplace=True)\n",
    "\n",
    "if runAll:\n",
    "    train_data_y = train_data['generated']\n",
    "    train_data.drop(columns=['generated', 'text', 'tokenized_text', 'pos_tags', 'text_for_tfidf'], inplace=True)\n",
    "\n",
    "    pretest_data_y = pretest_data['generated']\n",
    "    pretest_data.drop(columns=['generated', 'text', 'tokenized_text', 'pos_tags', 'text_for_tfidf'], inplace=True)\n",
    "\n",
    "    test_data_y = test_data['generated']\n",
    "    test_data.drop(columns=['generated', 'text', 'tokenized_text', 'pos_tags', 'text_for_tfidf'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_freq</th>\n",
       "      <th>determiner_freq</th>\n",
       "      <th>conjunction_freq</th>\n",
       "      <th>auxiliary_freq</th>\n",
       "      <th>av_sentence_length</th>\n",
       "      <th>vocabulary_diversity</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>zipfian_coefficient</th>\n",
       "      <th>dot_freq</th>\n",
       "      <th>comma_freq</th>\n",
       "      <th>question_freq</th>\n",
       "      <th>exclamation_freq</th>\n",
       "      <th>stopword_freq</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.390549</td>\n",
       "      <td>0.302293</td>\n",
       "      <td>0.588925</td>\n",
       "      <td>0.399887</td>\n",
       "      <td>0.119902</td>\n",
       "      <td>0.610375</td>\n",
       "      <td>0.947123</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080479</td>\n",
       "      <td>0.109873</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.569878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.396010</td>\n",
       "      <td>0.202539</td>\n",
       "      <td>0.500598</td>\n",
       "      <td>0.399517</td>\n",
       "      <td>0.068056</td>\n",
       "      <td>0.451015</td>\n",
       "      <td>0.909222</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.109873</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.263298</td>\n",
       "      <td>0.913896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440545</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>0.492153</td>\n",
       "      <td>0.334447</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.536569</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113870</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.231383</td>\n",
       "      <td>0.969648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.315471</td>\n",
       "      <td>0.264457</td>\n",
       "      <td>0.548116</td>\n",
       "      <td>0.374249</td>\n",
       "      <td>0.032212</td>\n",
       "      <td>0.565043</td>\n",
       "      <td>0.958272</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089897</td>\n",
       "      <td>0.140127</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.292553</td>\n",
       "      <td>0.793940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.364577</td>\n",
       "      <td>0.387015</td>\n",
       "      <td>0.099733</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>0.981160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.495223</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noun_freq  determiner_freq  conjunction_freq  auxiliary_freq  \\\n",
       "0   0.390549         0.302293          0.588925        0.399887   \n",
       "1   0.396010         0.202539          0.500598        0.399517   \n",
       "2   0.440545         0.209395          0.492153        0.334447   \n",
       "3   0.315471         0.264457          0.548116        0.374249   \n",
       "4   1.000000         0.364577          0.387015        0.099733   \n",
       "\n",
       "   av_sentence_length  vocabulary_diversity  flesch_reading_ease  \\\n",
       "0            0.119902              0.610375             0.947123   \n",
       "1            0.068056              0.451015             0.909222   \n",
       "2            0.028372              0.536569             0.930558   \n",
       "3            0.032212              0.565043             0.958272   \n",
       "4            0.007133              0.008434             0.981160   \n",
       "\n",
       "   zipfian_coefficient  dot_freq  comma_freq  question_freq  exclamation_freq  \\\n",
       "0             0.003019  0.028269    0.069565       0.060606               0.0   \n",
       "1             0.007246  0.021201    0.052174       0.000000               0.0   \n",
       "2             0.003451  0.049470    0.086957       0.000000               0.0   \n",
       "3             0.005368  0.031802    0.026087       0.000000               0.0   \n",
       "4             0.000000  0.226148    0.000000       0.030303               0.0   \n",
       "\n",
       "   stopword_freq       neg       neu       pos  compound  \n",
       "0       0.080479  0.109873  0.809524  0.140957  0.569878  \n",
       "1       0.095890  0.109873  0.736508  0.263298  0.913896  \n",
       "2       0.113870  0.070064  0.793651  0.231383  0.969648  \n",
       "3       0.089897  0.140127  0.688889  0.292553  0.793940  \n",
       "4       0.219178  0.495223  0.509524  0.000000  0.000200  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all features to be between 0 and 1\n",
    "# based on the training data (if runall)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "if not runAll:\n",
    "    valid_data = pd.DataFrame(scaler.fit_transform(valid_data), columns=valid_data.columns)\n",
    "\n",
    "if runAll:\n",
    "    train_data = pd.DataFrame(scaler.fit_transform(train_data), columns=train_data.columns)\n",
    "    pretest_data = pd.DataFrame(scaler.transform(pretest_data), columns=pretest_data.columns)\n",
    "    test_data = pd.DataFrame(scaler.transform(test_data), columns=test_data.columns)\n",
    "    valid_data = pd.DataFrame(scaler.transform(valid_data), columns=valid_data.columns)\n",
    "\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_freq</th>\n",
       "      <th>determiner_freq</th>\n",
       "      <th>conjunction_freq</th>\n",
       "      <th>auxiliary_freq</th>\n",
       "      <th>av_sentence_length</th>\n",
       "      <th>vocabulary_diversity</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>zipfian_coefficient</th>\n",
       "      <th>dot_freq</th>\n",
       "      <th>comma_freq</th>\n",
       "      <th>question_freq</th>\n",
       "      <th>exclamation_freq</th>\n",
       "      <th>stopword_freq</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.390549</td>\n",
       "      <td>0.302293</td>\n",
       "      <td>0.588925</td>\n",
       "      <td>0.399887</td>\n",
       "      <td>0.119902</td>\n",
       "      <td>0.610375</td>\n",
       "      <td>0.947123</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080479</td>\n",
       "      <td>0.109873</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.569878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.396010</td>\n",
       "      <td>0.202539</td>\n",
       "      <td>0.500598</td>\n",
       "      <td>0.399517</td>\n",
       "      <td>0.068056</td>\n",
       "      <td>0.451015</td>\n",
       "      <td>0.909222</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.109873</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.263298</td>\n",
       "      <td>0.913896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440545</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>0.492153</td>\n",
       "      <td>0.334447</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.536569</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113870</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.231383</td>\n",
       "      <td>0.969648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.315471</td>\n",
       "      <td>0.264457</td>\n",
       "      <td>0.548116</td>\n",
       "      <td>0.374249</td>\n",
       "      <td>0.032212</td>\n",
       "      <td>0.565043</td>\n",
       "      <td>0.958272</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089897</td>\n",
       "      <td>0.140127</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.292553</td>\n",
       "      <td>0.793940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.364577</td>\n",
       "      <td>0.387015</td>\n",
       "      <td>0.099733</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>0.981160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.495223</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noun_freq  determiner_freq  conjunction_freq  auxiliary_freq  \\\n",
       "0   0.390549         0.302293          0.588925        0.399887   \n",
       "1   0.396010         0.202539          0.500598        0.399517   \n",
       "2   0.440545         0.209395          0.492153        0.334447   \n",
       "3   0.315471         0.264457          0.548116        0.374249   \n",
       "4   1.000000         0.364577          0.387015        0.099733   \n",
       "\n",
       "   av_sentence_length  vocabulary_diversity  flesch_reading_ease  \\\n",
       "0            0.119902              0.610375             0.947123   \n",
       "1            0.068056              0.451015             0.909222   \n",
       "2            0.028372              0.536569             0.930558   \n",
       "3            0.032212              0.565043             0.958272   \n",
       "4            0.007133              0.008434             0.981160   \n",
       "\n",
       "   zipfian_coefficient  dot_freq  comma_freq  question_freq  exclamation_freq  \\\n",
       "0             0.003019  0.028269    0.069565       0.060606               0.0   \n",
       "1             0.007246  0.021201    0.052174       0.000000               0.0   \n",
       "2             0.003451  0.049470    0.086957       0.000000               0.0   \n",
       "3             0.005368  0.031802    0.026087       0.000000               0.0   \n",
       "4             0.000000  0.226148    0.000000       0.030303               0.0   \n",
       "\n",
       "   stopword_freq       neg       neu       pos  compound  \n",
       "0       0.080479  0.109873  0.809524  0.140957  0.569878  \n",
       "1       0.095890  0.109873  0.736508  0.263298  0.913896  \n",
       "2       0.113870  0.070064  0.793651  0.231383  0.969648  \n",
       "3       0.089897  0.140127  0.688889  0.292553  0.793940  \n",
       "4       0.219178  0.495223  0.509524  0.000000  0.000200  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the sparse matrix, DataFrames and feature names\n",
    "from scipy.sparse import save_npz\n",
    "import pickle\n",
    "\n",
    "save_npz('data/valid_data_x_sparse.npz', valid_data_sparse)\n",
    "valid_data.to_csv('data/valid_data_x_dense.csv', index=False)\n",
    "valid_data_y.to_csv('data/valid_data_y.csv', index=False)\n",
    "\n",
    "with open('data/sparse_matrices_feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(sparse_matrices_feature_names, f)\n",
    "\n",
    "if runAll:\n",
    "    save_npz('data/train_data_x_sparse.npz', train_data_sparse)\n",
    "    train_data.to_csv('data/train_data_x_dense.csv', index=False)\n",
    "    train_data_y.to_csv('data/train_data_y.csv', index=False)\n",
    "\n",
    "    save_npz('data/pretest_data_x_sparse.npz', pretest_data_sparse)\n",
    "    pretest_data.to_csv('data/pretest_data_x_dense.csv', index=False)\n",
    "    pretest_data_y.to_csv('pretest_data_y.csv', index=False)\n",
    "\n",
    "    save_npz('data/test_data_x_sparse.npz', test_data_sparse)\n",
    "    test_data.to_csv('data/test_data_x_dense.csv', index=False)\n",
    "    test_data_y.to_csv('data/test_data_y.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to concatinate data from saved .npz and .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1679, 5017)\n",
      "(1679, 1)\n",
      "(5000,)\n"
     ]
    }
   ],
   "source": [
    "# Use this cell to load datasets and make ready for ML\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "import pickle\n",
    "\n",
    "valid_data_x_sparse = load_npz('data/valid_data_x_sparse.npz')\n",
    "valid_data_x_dense = pd.read_csv('data/valid_data_x_dense.csv')\n",
    "\n",
    "with open('data/sparse_matrices_feature_names.pkl', 'rb') as f:\n",
    "    sparse_matrices_feature_names = pickle.load(f)\n",
    "\n",
    "# Combine the loaded sparse matrix with the dense matrix\n",
    "valid_data_x = hstack([valid_data_x_sparse, csr_matrix(valid_data_x_dense.values)]).toarray()\n",
    "valid_data_y = pd.read_csv('data/valid_data_y.csv')\n",
    "\n",
    "print(valid_data_x.shape)\n",
    "print(valid_data_y.shape)\n",
    "print(sparse_matrices_feature_names.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
