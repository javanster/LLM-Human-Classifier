{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size before removing rows identical to the training data: 73573\n",
      "Train and pretest data size: 158294\n",
      "Validation data size: 1679\n",
      "Test data size: 40202\n",
      "The valdation data is 0.84% of the total data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_and_pretest_data = pd.read_csv('data/detect_ai.csv')\n",
    "valid_data = pd.read_csv('data/detect_ai_validation.csv')\n",
    "test_data = pd.read_csv('data/daigt_v4.csv')\n",
    "\n",
    "print(f\"Test data size before removing rows identical to the training data: {len(test_data)}\")\n",
    "\n",
    "train_and_pretest_data = train_and_pretest_data.drop_duplicates(subset='text')\n",
    "test_data = test_data[~test_data['text'].isin(train_and_pretest_data['text'])]\n",
    "\n",
    "print(f\"Train and pretest data size: {len(train_and_pretest_data)}\")\n",
    "print(f\"Validation data size: {len(valid_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n",
    "\n",
    "valid_data_percentage = len(valid_data) / (len(train_and_pretest_data) + len(test_data) + len(valid_data))\n",
    "\n",
    "print(f\"The valdation data is {valid_data_percentage * 100:.2f}% of the total data\")\n",
    "\n",
    "# start a timer such that we know how the entire notebook takes to run\n",
    "import time\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change this variable to True if you want to process all 3 datasets\n",
    "Expected runtime if True: 3h\n",
    "\n",
    "Expected runtime if False: 2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "runAll = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and pretest data columns: Index(['id', 'prompt_id', 'text', 'generated'], dtype='object')\n",
      "Validation data columns: Index(['id', 'prompt_id', 'text', 'generated'], dtype='object')\n",
      "Test data columns: Index(['text', 'label', 'prompt_name', 'source', 'RDizzl3_seven', 'model'], dtype='object')\n",
      "Test data[model] has the following values: ['human' 'mistral' 'llama' 'gpt' 'claude' 'falcon' 'palm' 'cohere' 'ada'\n",
      " 'babbage' 'curie' 'davinci']\n"
     ]
    }
   ],
   "source": [
    "# Print the name of the columns in the dfs\n",
    "print(f\"Train and pretest data columns: {train_and_pretest_data.columns}\")\n",
    "print(f\"Validation data columns: {valid_data.columns}\")\n",
    "print(f\"Test data columns: {test_data.columns}\")\n",
    "\n",
    "# print what values test_data[model]] has\n",
    "print(f\"Test data[model] has the following values: {test_data['model'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and pretest data columns: Index(['text', 'generated'], dtype='object')\n",
      "Validation data columns: Index(['text', 'generated'], dtype='object')\n",
      "Test data columns: Index(['text', 'generated'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In recent years, there has been a growing move...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>---\\nWhy not cars in our life\\n===============...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A car is considered by many a nessecity for ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H\\n\\nello fellow citezens , we are here to inf...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have you ever known how if feels not being abl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  generated\n",
       "0  In recent years, there has been a growing move...          1\n",
       "1  ---\\nWhy not cars in our life\\n===============...          1\n",
       "2  A car is considered by many a nessecity for ev...          1\n",
       "3  H\\n\\nello fellow citezens , we are here to inf...          0\n",
       "4  Have you ever known how if feels not being abl...          1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Irrelevant Data\n",
    "# and renaming the columns to be the same for all datasets\n",
    "# to ['text', 'generated'] as string, boolean\n",
    "\n",
    "train_and_pretest_data = train_and_pretest_data[['text', 'generated']]\n",
    "valid_data = valid_data[['text', 'generated']]\n",
    "\n",
    "\n",
    "# For the Test dataset\n",
    "test_data['generated'] = (test_data['model'] != 'human').astype(int)\n",
    "test_data = test_data[['text', 'generated']]\n",
    "\n",
    "print(f\"Train and pretest data columns: {train_and_pretest_data.columns}\")\n",
    "print(f\"Validation data columns: {valid_data.columns}\")\n",
    "print(f\"Test data columns: {test_data.columns}\")\n",
    "\n",
    "train_and_pretest_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # download these if it's your first time running the code\\nimport nltk\\nnltk.download('punkt')\\nnltk.download('wordnet')\\nnltk.download('stopwords')\\nnltk.download('averaged_perceptron_tagger')\\nnltk.download('vader_lexicon')\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # download these if it's your first time running the code\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from nltk import word_tokenize\n",
    "\n",
    "valid_data['tokenized_text'] = valid_data['text'].apply(word_tokenize)\n",
    "if runAll:\n",
    "    train_and_pretest_data['tokenized_text'] = train_and_pretest_data['text'].apply(word_tokenize)\n",
    "    test_data['tokenized_text'] = test_data['text'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "\n",
    "valid_data['tokenized_text'] = valid_data['tokenized_text'].apply(lambda x: [word.lower() for word in x])\n",
    "if runAll:\n",
    "    train_and_pretest_data['tokenized_text'] = train_and_pretest_data['tokenized_text'].apply(lambda x: [word.lower() for word in x])\n",
    "    test_data['tokenized_text'] = test_data['tokenized_text'].apply(lambda x: [word.lower() for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>noun_freq</th>\n",
       "      <th>determiner_freq</th>\n",
       "      <th>conjunction_freq</th>\n",
       "      <th>auxiliary_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_,_ _and it has to do with the fact that if yo...</td>\n",
       "      <td>1</td>\n",
       "      <td>[_, ,, _, _and, it, has, to, do, with, the, fa...</td>\n",
       "      <td>[(_, NN), (,, ,), (_, FW), (_and, NN), (it, PR...</td>\n",
       "      <td>0.194175</td>\n",
       "      <td>0.082524</td>\n",
       "      <td>0.150485</td>\n",
       "      <td>0.228155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>There are advantages of limiting car usage les...</td>\n",
       "      <td>1</td>\n",
       "      <td>[there, are, advantages, of, limiting, car, us...</td>\n",
       "      <td>[(there, EX), (are, VBP), (advantages, NNS), (...</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Limiting car usage ii very beneifial to not on...</td>\n",
       "      <td>1</td>\n",
       "      <td>[limiting, car, usage, ii, very, beneifial, to...</td>\n",
       "      <td>[(limiting, VBG), (car, NN), (usage, NN), (ii,...</td>\n",
       "      <td>0.210884</td>\n",
       "      <td>0.057823</td>\n",
       "      <td>0.125850</td>\n",
       "      <td>0.200680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cars have been one of the most advanced invent...</td>\n",
       "      <td>1</td>\n",
       "      <td>[cars, have, been, one, of, the, most, advance...</td>\n",
       "      <td>[(cars, NNS), (have, VBP), (been, VBN), (one, ...</td>\n",
       "      <td>0.169082</td>\n",
       "      <td>0.072464</td>\n",
       "      <td>0.140097</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Are cars even really necessary? Vehicles can c...</td>\n",
       "      <td>1</td>\n",
       "      <td>[are, cars, even, really, necessary, ?, vehicl...</td>\n",
       "      <td>[(are, VBP), (cars, NNS), (even, RB), (really,...</td>\n",
       "      <td>0.397866</td>\n",
       "      <td>0.099085</td>\n",
       "      <td>0.099085</td>\n",
       "      <td>0.102134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  generated  \\\n",
       "0  _,_ _and it has to do with the fact that if yo...          1   \n",
       "1  There are advantages of limiting car usage les...          1   \n",
       "2  Limiting car usage ii very beneifial to not on...          1   \n",
       "3  Cars have been one of the most advanced invent...          1   \n",
       "4  Are cars even really necessary? Vehicles can c...          1   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [_, ,, _, _and, it, has, to, do, with, the, fa...   \n",
       "1  [there, are, advantages, of, limiting, car, us...   \n",
       "2  [limiting, car, usage, ii, very, beneifial, to...   \n",
       "3  [cars, have, been, one, of, the, most, advance...   \n",
       "4  [are, cars, even, really, necessary, ?, vehicl...   \n",
       "\n",
       "                                            pos_tags  noun_freq  \\\n",
       "0  [(_, NN), (,, ,), (_, FW), (_and, NN), (it, PR...   0.194175   \n",
       "1  [(there, EX), (are, VBP), (advantages, NNS), (...   0.196000   \n",
       "2  [(limiting, VBG), (car, NN), (usage, NN), (ii,...   0.210884   \n",
       "3  [(cars, NNS), (have, VBP), (been, VBN), (one, ...   0.169082   \n",
       "4  [(are, VBP), (cars, NNS), (even, RB), (really,...   0.397866   \n",
       "\n",
       "   determiner_freq  conjunction_freq  auxiliary_freq  \n",
       "0         0.082524          0.150485        0.228155  \n",
       "1         0.056000          0.128000        0.228000  \n",
       "2         0.057823          0.125850        0.200680  \n",
       "3         0.072464          0.140097        0.217391  \n",
       "4         0.099085          0.099085        0.102134  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Syntax and Grammar Patterns\n",
    "from nltk import pos_tag\n",
    "from collections import Counter\n",
    "\n",
    "valid_data['pos_tags'] = valid_data['tokenized_text'].apply(pos_tag)\n",
    "\n",
    "if runAll:\n",
    "    train_and_pretest_data['pos_tags'] = train_and_pretest_data['tokenized_text'].apply(pos_tag)\n",
    "    test_data['pos_tags'] = test_data['tokenized_text'].apply(pos_tag)\n",
    "\n",
    "\"\"\"\n",
    "From the article: Part-of-speech analysis empha-\n",
    "sizes the dominance of nouns in ChatGPT texts, implying\n",
    "\n",
    "argumentativeness and objectivity, while the dependency\n",
    "\n",
    "parsing analysis shows that ChatGPT texts use more deter-\n",
    "miners, conjunctions, and auxiliary relations [18].\n",
    "\"\"\"\n",
    "\n",
    "# Function to count specific POS tags and normalize by text length\n",
    "def add_normalized_pos_features(df):\n",
    "    def normalize_pos_counts(text_tokens):\n",
    "        total_words = len(text_tokens)\n",
    "        if total_words == 0:\n",
    "            return {'noun_freq': 0, 'determiner_freq': 0, 'conjunction_freq': 0, 'auxiliary_freq': 0}\n",
    "        \n",
    "        tags = pos_tag(text_tokens)\n",
    "        tag_counts = Counter(tag for word, tag in tags)\n",
    "        noun_freq = sum(tag_counts[tag] for tag in ['NN', 'NNS', 'NNP', 'NNPS']) / total_words\n",
    "        determiner_freq = tag_counts['DT'] / total_words\n",
    "        conjunction_freq = sum(tag_counts[tag] for tag in ['CC', 'IN']) / total_words\n",
    "        auxiliary_freq = sum(tag_counts[tag] for tag in ['MD', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']) / total_words\n",
    "\n",
    "        return {\n",
    "            'noun_freq': noun_freq,\n",
    "            'determiner_freq': determiner_freq,\n",
    "            'conjunction_freq': conjunction_freq,\n",
    "            'auxiliary_freq': auxiliary_freq\n",
    "        }\n",
    "\n",
    "    pos_features = df['tokenized_text'].apply(normalize_pos_counts)\n",
    "    pos_features_df = pos_features.apply(pd.Series)\n",
    "    for column in pos_features_df.columns:\n",
    "        df[column] = pos_features_df[column]\n",
    "\n",
    "# Applying the function to the DataFrame\n",
    "add_normalized_pos_features(valid_data)\n",
    "if runAll:\n",
    "    add_normalized_pos_features(train_and_pretest_data)\n",
    "    add_normalized_pos_features(test_data)\n",
    "\n",
    "# Example to view the result\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>av_sentence_length</th>\n",
       "      <th>vocabulary_diversity</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>zipfian_coefficient</th>\n",
       "      <th>dot_freq</th>\n",
       "      <th>comma_freq</th>\n",
       "      <th>question_freq</th>\n",
       "      <th>exclamation_freq</th>\n",
       "      <th>stopword_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.666667</td>\n",
       "      <td>0.577670</td>\n",
       "      <td>77.77</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.666667</td>\n",
       "      <td>0.432000</td>\n",
       "      <td>57.27</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>68.81</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>83.80</td>\n",
       "      <td>1.222222</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.939394</td>\n",
       "      <td>0.027439</td>\n",
       "      <td>96.18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   av_sentence_length  vocabulary_diversity  flesch_reading_ease  \\\n",
       "0           68.666667              0.577670                77.77   \n",
       "1           41.666667              0.432000                57.27   \n",
       "2           21.000000              0.510204                68.81   \n",
       "3           23.000000              0.536232                83.80   \n",
       "4            9.939394              0.027439                96.18   \n",
       "\n",
       "   zipfian_coefficient  dot_freq  comma_freq  question_freq  exclamation_freq  \\\n",
       "0             1.125000         8           8              2                 0   \n",
       "1             1.300000         6           6              0                 0   \n",
       "2             1.142857        14          10              0                 0   \n",
       "3             1.222222         9           3              0                 0   \n",
       "4             1.000000        64           0              1                 0   \n",
       "\n",
       "   stopword_freq  \n",
       "0            100  \n",
       "1            118  \n",
       "2            139  \n",
       "3            111  \n",
       "4            262  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Complexity and Diversity\n",
    "\"\"\"Features like sentence length,\n",
    "lexical diversity, sentiment analysis,\n",
    "and complexity of ideas could also be\n",
    "indicative of the source of the text.\"\"\"\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "\n",
    "# Function to calculate average sentence length in words\n",
    "def average_sentence_length(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if sentences:  # Check if there are any sentences to avoid division by zero\n",
    "        return sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
    "    else:\n",
    "        return 0  # Return 0 if text contains no sentences\n",
    "\n",
    "valid_data['av_sentence_length'] = valid_data['text'].apply(average_sentence_length)\n",
    "\n",
    "if runAll:\n",
    "    train_and_pretest_data['av_sentence_length'] = train_and_pretest_data['text'].apply(average_sentence_length)\n",
    "    test_data['av_sentence_length'] = test_data['text'].apply(average_sentence_length)\n",
    "\n",
    "\n",
    "# Lexical diversity\n",
    "from nltk import FreqDist\n",
    "\n",
    "valid_data['vocabulary_diversity'] = valid_data['tokenized_text'].apply(lambda x: len(set(x)) / len(x))\n",
    "if runAll:\n",
    "    train_and_pretest_data['vocabulary_diversity'] = train_and_pretest_data['tokenized_text'].apply(lambda x: len(set(x)) / len(x))\n",
    "    test_data['vocabulary_diversity'] = test_data['tokenized_text'].apply(lambda x: len(set(x)) / len(x))\n",
    "\n",
    "\n",
    "# Readability\n",
    "\"\"\"The Flesch Reading Ease score is a\n",
    "readability test that provides a numerical\n",
    "score indicating how easy or difficult a\n",
    "text is to understand. The score is calculated\n",
    "based on the average length of sentences and\n",
    "the average number of syllables per word in the text.\n",
    "Scores typically range from 0 to 100,\n",
    "with higher scores indicating easier readability.\"\"\"\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "valid_data['flesch_reading_ease'] = valid_data['text'].apply(flesch_reading_ease)\n",
    "if runAll:\n",
    "    train_and_pretest_data['flesch_reading_ease'] = train_and_pretest_data['text'].apply(flesch_reading_ease)\n",
    "    test_data['flesch_reading_ease'] = test_data['text'].apply(flesch_reading_ease)\n",
    "\n",
    "\n",
    "# Zipfian Coefficient\n",
    "\"\"\"The Zipfian coefficient is a measure of the\n",
    "distribution of word frequencies in a text.\n",
    "It is calculated by plotting the frequency of\n",
    "each word in the text against its rank in the\n",
    "frequency table and fitting a curve to the data.\n",
    "\"\"\"\n",
    "from nltk import ngrams\n",
    "\n",
    "def zipfian_coefficient(text, n=1):\n",
    "    ngrams_list = list(ngrams(text, n))\n",
    "    freq_dist = FreqDist(ngrams_list)\n",
    "    freq_values = list(freq_dist.values())\n",
    "    freq_values.sort(reverse=True)\n",
    "    return freq_values[0] / freq_values[1]\n",
    "\n",
    "valid_data['zipfian_coefficient'] = valid_data['tokenized_text'].apply(zipfian_coefficient)\n",
    "if runAll:\n",
    "    train_and_pretest_data['zipfian_coefficient'] = train_and_pretest_data['tokenized_text'].apply(zipfian_coefficient)\n",
    "    test_data['zipfian_coefficient'] = test_data['tokenized_text'].apply(zipfian_coefficient)\n",
    "\n",
    "\n",
    "# Non-character analysis\n",
    "# Create features for the frequency of dots, commas, questions marks and exclamations marks in the text\n",
    "\n",
    "def add_punctuation_features(df):\n",
    "    df['dot_freq'] = df['text'].str.count('\\\\.')\n",
    "    df['comma_freq'] = df['text'].str.count(',')\n",
    "    df['question_freq'] = df['text'].str.count('\\\\?')\n",
    "    df['exclamation_freq'] = df['text'].str.count('!')\n",
    "\n",
    "add_punctuation_features(valid_data)\n",
    "if runAll:\n",
    "    add_punctuation_features(train_and_pretest_data)\n",
    "    add_punctuation_features(test_data)\n",
    "\n",
    "\n",
    "# Stopword Frequency\n",
    "from nltk.corpus import stopwords\n",
    "# Create features for the frequency of stopwords\n",
    "def add_stopword_count_feature(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['stopword_freq'] = df['tokenized_text'].apply(lambda x: sum(1 for word in x if word in stop_words))\n",
    "\n",
    "add_stopword_count_feature(valid_data)\n",
    "if runAll:\n",
    "    add_stopword_count_feature(train_and_pretest_data)\n",
    "    add_stopword_count_feature(test_data)\n",
    "\n",
    "\n",
    "# print the head of the new features for valid_data\n",
    "valid_data[['av_sentence_length', 'vocabulary_diversity', 'flesch_reading_ease', 'zipfian_coefficient', 'dot_freq', 'comma_freq', 'question_freq', 'exclamation_freq', 'stopword_freq']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_data['tokenized_text'] after removing stopwords: 0    [_, ,, _, _and, fact, automobile, ,, danger, g...\n",
      "1    [advantages, limiting, car, usage, less, green...\n",
      "2    [limiting, car, usage, ii, beneifial, envirome...\n",
      "3    [cars, one, advanced, invention, world, ;, las...\n",
      "4    [cars, even, really, necessary, ?, vehicles, c...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Stop word removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Removing stopwords from the text\n",
    "valid_data['tokenized_text'] = valid_data['tokenized_text'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
    "if runAll:\n",
    "    train_and_pretest_data['tokenized_text'] = train_and_pretest_data['tokenized_text'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
    "    test_data['tokenized_text'] = test_data['tokenized_text'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
    "\n",
    "print(f\"valid_data['tokenized_text'] after removing stopwords: {valid_data['tokenized_text'].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_data['tokenized_text'] after lemmatization: 0    [_, ,, _, _and, fact, automobile, ,, danger, g...\n",
      "1    [advantage, limiting, car, usage, le, greenhou...\n",
      "2    [limiting, car, usage, ii, beneifial, envirome...\n",
      "3    [car, one, advanced, invention, world, ;, last...\n",
      "4    [car, even, really, necessary, ?, vehicle, cau...\n",
      "Name: tokenized_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize the text\n",
    "valid_data['tokenized_text'] = valid_data['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "if runAll:\n",
    "    train_and_pretest_data['tokenized_text'] = train_and_pretest_data['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "    test_data['tokenized_text'] = test_data['tokenized_text'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "\n",
    "print(f\"valid_data['tokenized_text'] after lemmatization: {valid_data['tokenized_text'].head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.1397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.8277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.044</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.9392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.088</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.5878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.311</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.9996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     neg    neu    pos  compound\n",
       "0  0.069  0.878  0.053    0.1397\n",
       "1  0.069  0.832  0.099    0.8277\n",
       "2  0.044  0.868  0.087    0.9392\n",
       "3  0.088  0.802  0.110    0.5878\n",
       "4  0.311  0.689  0.000   -0.9996"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "valid_data['sentiment_scores'] = valid_data['text'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x))\n",
    "sentiment_columns = valid_data['sentiment_scores'].apply(pd.Series)\n",
    "valid_data = pd.concat([valid_data, sentiment_columns], axis=1)\n",
    "valid_data.drop('sentiment_scores', axis=1, inplace=True)\n",
    "\n",
    "if runAll:\n",
    "    train_and_pretest_data['sentiment_scores'] = train_and_pretest_data['text'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x))\n",
    "    sentiment_columns = train_and_pretest_data['sentiment_scores'].apply(pd.Series)\n",
    "    train_and_pretest_data = pd.concat([train_and_pretest_data, sentiment_columns], axis=1)\n",
    "    train_and_pretest_data.drop('sentiment_scores', axis=1, inplace=True)\n",
    "\n",
    "    test_data['sentiment_scores'] = test_data['text'].apply(lambda x: SentimentIntensityAnalyzer().polarity_scores(x))\n",
    "    sentiment_columns = test_data['sentiment_scores'].apply(pd.Series)\n",
    "    test_data = pd.concat([test_data, sentiment_columns], axis=1)\n",
    "    test_data.drop('sentiment_scores', axis=1, inplace=True)\n",
    "\n",
    "valid_data[['neg', 'neu', 'pos', 'compound']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1679, 5000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Limit to 5000 features\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "valid_data['text_for_tfidf'] = valid_data['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "valid_data_sparse = vectorizer.fit_transform(valid_data['text_for_tfidf'])\n",
    "\n",
    "if runAll:\n",
    "    train_and_pretest_data['text_for_tfidf'] = train_and_pretest_data['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "    train_and_pretest_data_sparse = vectorizer.fit_transform(train_and_pretest_data['text_for_tfidf'])\n",
    "\n",
    "    test_data['text_for_tfidf'] = test_data['tokenized_text'].apply(lambda x: ' '.join(x))\n",
    "    test_data_sparse = vectorizer.fit_transform(test_data['text_for_tfidf'])\n",
    "\n",
    "valid_data_sparse.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_matrix.to_csv('tfidf_matrix.csv', index=False)\n",
    "# This matrix was 115 MB, saving sparse matrices as csv will be unfeasable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns and split into x and y\n",
    "\n",
    "valid_data_y = valid_data['generated']\n",
    "valid_data.drop(columns=['generated', 'text', 'tokenized_text', 'pos_tags', 'text_for_tfidf'], inplace=True)\n",
    "\n",
    "if runAll:\n",
    "    train_and_pretest_data_y = train_and_pretest_data['generated']\n",
    "    train_and_pretest_data.drop(columns=['generated', 'text', 'tokenized_text', 'pos_tags', 'text_for_tfidf'], inplace=True)\n",
    "\n",
    "    test_data_y = test_data['generated']\n",
    "    test_data.drop(columns=['generated', 'text', 'tokenized_text', 'pos_tags', 'text_for_tfidf'], inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_freq</th>\n",
       "      <th>determiner_freq</th>\n",
       "      <th>conjunction_freq</th>\n",
       "      <th>auxiliary_freq</th>\n",
       "      <th>av_sentence_length</th>\n",
       "      <th>vocabulary_diversity</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>zipfian_coefficient</th>\n",
       "      <th>dot_freq</th>\n",
       "      <th>comma_freq</th>\n",
       "      <th>question_freq</th>\n",
       "      <th>exclamation_freq</th>\n",
       "      <th>stopword_freq</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.390549</td>\n",
       "      <td>0.302293</td>\n",
       "      <td>0.588925</td>\n",
       "      <td>0.399887</td>\n",
       "      <td>0.119902</td>\n",
       "      <td>0.610375</td>\n",
       "      <td>0.947123</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080479</td>\n",
       "      <td>0.109873</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.569878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.396010</td>\n",
       "      <td>0.202539</td>\n",
       "      <td>0.500598</td>\n",
       "      <td>0.399517</td>\n",
       "      <td>0.068056</td>\n",
       "      <td>0.451015</td>\n",
       "      <td>0.909222</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.109873</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.263298</td>\n",
       "      <td>0.913896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440545</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>0.492153</td>\n",
       "      <td>0.334447</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.536569</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113870</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.231383</td>\n",
       "      <td>0.969648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.315471</td>\n",
       "      <td>0.264457</td>\n",
       "      <td>0.548116</td>\n",
       "      <td>0.374249</td>\n",
       "      <td>0.032212</td>\n",
       "      <td>0.565043</td>\n",
       "      <td>0.958272</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089897</td>\n",
       "      <td>0.140127</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.292553</td>\n",
       "      <td>0.793940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.364577</td>\n",
       "      <td>0.387015</td>\n",
       "      <td>0.099733</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>0.981160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.495223</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noun_freq  determiner_freq  conjunction_freq  auxiliary_freq  \\\n",
       "0   0.390549         0.302293          0.588925        0.399887   \n",
       "1   0.396010         0.202539          0.500598        0.399517   \n",
       "2   0.440545         0.209395          0.492153        0.334447   \n",
       "3   0.315471         0.264457          0.548116        0.374249   \n",
       "4   1.000000         0.364577          0.387015        0.099733   \n",
       "\n",
       "   av_sentence_length  vocabulary_diversity  flesch_reading_ease  \\\n",
       "0            0.119902              0.610375             0.947123   \n",
       "1            0.068056              0.451015             0.909222   \n",
       "2            0.028372              0.536569             0.930558   \n",
       "3            0.032212              0.565043             0.958272   \n",
       "4            0.007133              0.008434             0.981160   \n",
       "\n",
       "   zipfian_coefficient  dot_freq  comma_freq  question_freq  exclamation_freq  \\\n",
       "0             0.003019  0.028269    0.069565       0.060606               0.0   \n",
       "1             0.007246  0.021201    0.052174       0.000000               0.0   \n",
       "2             0.003451  0.049470    0.086957       0.000000               0.0   \n",
       "3             0.005368  0.031802    0.026087       0.000000               0.0   \n",
       "4             0.000000  0.226148    0.000000       0.030303               0.0   \n",
       "\n",
       "   stopword_freq       neg       neu       pos  compound  \n",
       "0       0.080479  0.109873  0.809524  0.140957  0.569878  \n",
       "1       0.095890  0.109873  0.736508  0.263298  0.913896  \n",
       "2       0.113870  0.070064  0.793651  0.231383  0.969648  \n",
       "3       0.089897  0.140127  0.688889  0.292553  0.793940  \n",
       "4       0.219178  0.495223  0.509524  0.000000  0.000200  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize all features to be between 0 and 1\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "if not runAll:\n",
    "    valid_data = pd.DataFrame(MinMaxScaler().fit_transform(valid_data), columns=valid_data.columns)\n",
    "\n",
    "if runAll:\n",
    "    # Instead, normalize train_and_pretest_data_y dataset and use those values to normalize the other datasets\n",
    "    train_and_pretest_data = pd.DataFrame(MinMaxScaler().fit_transform(train_and_pretest_data), columns=train_and_pretest_data.columns)\n",
    "    test_data = pd.DataFrame(MinMaxScaler().fit_transform(test_data), columns=test_data.columns)\n",
    "    valid_data = pd.DataFrame(MinMaxScaler().fit_transform(valid_data), columns=valid_data.columns)\n",
    "\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun_freq</th>\n",
       "      <th>determiner_freq</th>\n",
       "      <th>conjunction_freq</th>\n",
       "      <th>auxiliary_freq</th>\n",
       "      <th>av_sentence_length</th>\n",
       "      <th>vocabulary_diversity</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>zipfian_coefficient</th>\n",
       "      <th>dot_freq</th>\n",
       "      <th>comma_freq</th>\n",
       "      <th>question_freq</th>\n",
       "      <th>exclamation_freq</th>\n",
       "      <th>stopword_freq</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.390549</td>\n",
       "      <td>0.302293</td>\n",
       "      <td>0.588925</td>\n",
       "      <td>0.399887</td>\n",
       "      <td>0.119902</td>\n",
       "      <td>0.610375</td>\n",
       "      <td>0.947123</td>\n",
       "      <td>0.003019</td>\n",
       "      <td>0.028269</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080479</td>\n",
       "      <td>0.109873</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.140957</td>\n",
       "      <td>0.569878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.396010</td>\n",
       "      <td>0.202539</td>\n",
       "      <td>0.500598</td>\n",
       "      <td>0.399517</td>\n",
       "      <td>0.068056</td>\n",
       "      <td>0.451015</td>\n",
       "      <td>0.909222</td>\n",
       "      <td>0.007246</td>\n",
       "      <td>0.021201</td>\n",
       "      <td>0.052174</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>0.109873</td>\n",
       "      <td>0.736508</td>\n",
       "      <td>0.263298</td>\n",
       "      <td>0.913896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.440545</td>\n",
       "      <td>0.209395</td>\n",
       "      <td>0.492153</td>\n",
       "      <td>0.334447</td>\n",
       "      <td>0.028372</td>\n",
       "      <td>0.536569</td>\n",
       "      <td>0.930558</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113870</td>\n",
       "      <td>0.070064</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.231383</td>\n",
       "      <td>0.969648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.315471</td>\n",
       "      <td>0.264457</td>\n",
       "      <td>0.548116</td>\n",
       "      <td>0.374249</td>\n",
       "      <td>0.032212</td>\n",
       "      <td>0.565043</td>\n",
       "      <td>0.958272</td>\n",
       "      <td>0.005368</td>\n",
       "      <td>0.031802</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089897</td>\n",
       "      <td>0.140127</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.292553</td>\n",
       "      <td>0.793940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.364577</td>\n",
       "      <td>0.387015</td>\n",
       "      <td>0.099733</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>0.981160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.495223</td>\n",
       "      <td>0.509524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noun_freq  determiner_freq  conjunction_freq  auxiliary_freq  \\\n",
       "0   0.390549         0.302293          0.588925        0.399887   \n",
       "1   0.396010         0.202539          0.500598        0.399517   \n",
       "2   0.440545         0.209395          0.492153        0.334447   \n",
       "3   0.315471         0.264457          0.548116        0.374249   \n",
       "4   1.000000         0.364577          0.387015        0.099733   \n",
       "\n",
       "   av_sentence_length  vocabulary_diversity  flesch_reading_ease  \\\n",
       "0            0.119902              0.610375             0.947123   \n",
       "1            0.068056              0.451015             0.909222   \n",
       "2            0.028372              0.536569             0.930558   \n",
       "3            0.032212              0.565043             0.958272   \n",
       "4            0.007133              0.008434             0.981160   \n",
       "\n",
       "   zipfian_coefficient  dot_freq  comma_freq  question_freq  exclamation_freq  \\\n",
       "0             0.003019  0.028269    0.069565       0.060606               0.0   \n",
       "1             0.007246  0.021201    0.052174       0.000000               0.0   \n",
       "2             0.003451  0.049470    0.086957       0.000000               0.0   \n",
       "3             0.005368  0.031802    0.026087       0.000000               0.0   \n",
       "4             0.000000  0.226148    0.000000       0.030303               0.0   \n",
       "\n",
       "   stopword_freq       neg       neu       pos  compound  \n",
       "0       0.080479  0.109873  0.809524  0.140957  0.569878  \n",
       "1       0.095890  0.109873  0.736508  0.263298  0.913896  \n",
       "2       0.113870  0.070064  0.793651  0.231383  0.969648  \n",
       "3       0.089897  0.140127  0.688889  0.292553  0.793940  \n",
       "4       0.219178  0.495223  0.509524  0.000000  0.000200  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the sparse matrix and DataFrames\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "save_npz('valid_data_x_sparse.npz', valid_data_sparse)\n",
    "valid_data.to_csv('valid_data_x_dense.csv', index=False)\n",
    "valid_data_y.to_csv('valid_data_y.csv', index=False)\n",
    "\n",
    "if runAll:\n",
    "    save_npz('train_and_pretest_data_x_sparse.npz', train_and_pretest_data_sparse)\n",
    "    train_and_pretest_data.to_csv('train_and_pretest_data_x_dense.csv', index=False)\n",
    "    train_and_pretest_data_y.to_csv('train_and_pretest_data_y.csv', index=False)\n",
    "\n",
    "    save_npz('test_data_x_sparse.npz', test_data_sparse)\n",
    "    test_data.to_csv('test_data_x_dense.csv', index=False)\n",
    "    test_data_y.to_csv('test_data_y.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "valid_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to run the notebook: 89.45 seconds\n",
      "\n",
      "Time it will take to run the entire notebook for all the data:\n",
      "2 hours, 57 minutes, and 44.44 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# stop and print timer such that we know how the entire notebook takes to run\n",
    "end = time.time()\n",
    "print(f\"Time taken to run the notebook: {end - start:.2f} seconds\")\n",
    "\n",
    "total_seconds = (end - start) / valid_data_percentage\n",
    "hours = int(total_seconds // 3600)\n",
    "minutes = int((total_seconds % 3600) // 60)\n",
    "seconds = total_seconds % 60\n",
    "\n",
    "print(f\"\\nTime it will take to run the entire notebook for all the data:\\n{hours} hours, {minutes} minutes, and {seconds:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to concatinate data from saved .npz and .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1679, 5017)\n",
      "(1679, 1)\n"
     ]
    }
   ],
   "source": [
    "# Use this window\n",
    "# to load datasets and make ready for ML\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.sparse import load_npz\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "valid_data_x_sparse = load_npz('valid_data_x_sparse.npz')\n",
    "valid_data_x_dense = pd.read_csv('valid_data_x_dense.csv')\n",
    "\n",
    "# Combine the loaded sparse matrix with the additional features\n",
    "valid_data_x = hstack([valid_data_x_sparse, csr_matrix(valid_data_x_dense.values)]).toarray()\n",
    "valid_data_y = pd.read_csv('valid_data_y.csv')\n",
    "\n",
    "print(valid_data_x.shape)\n",
    "print(valid_data_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of how to use ML algorithm for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\.venv\\Lib\\site-packages\\sklearn\\base.py:1474: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7678571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        78\n",
      "           1       0.77      1.00      0.87       258\n",
      "\n",
      "    accuracy                           0.77       336\n",
      "   macro avg       0.38      0.50      0.43       336\n",
      "weighted avg       0.59      0.77      0.67       336\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Projects\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split data (this will be done differently\n",
    "# when dealing with all 3 datasets, not just valid_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    valid_data_x, valid_data_y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
