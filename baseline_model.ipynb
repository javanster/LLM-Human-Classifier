{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and pretest data size: 158294\n",
      "Test data size: 40202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_and_pretest_data = pd.read_csv('data/detect_ai.csv')\n",
    "test_data = pd.read_csv('data/daigt_v4.csv')\n",
    "\n",
    "train_and_pretest_data = train_and_pretest_data.drop_duplicates(subset='text')\n",
    "test_data = test_data[~test_data['text'].isin(train_and_pretest_data['text'])]\n",
    "\n",
    "print(f\"Train and pretest data size: {len(train_and_pretest_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_data(data):\n",
    "    tokens = word_tokenize(data.lower())\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "train_pretest_tokenized_df = pd.DataFrame(columns=['tokens', 'label'])\n",
    "test_tokenized_df = pd.DataFrame(columns=['tokens', 'label'])\n",
    "\n",
    "for index, row in train_and_pretest_data.iterrows():\n",
    "    train_pretest_tokenized_df = pd.concat([train_pretest_tokenized_df, pd.DataFrame({'tokens': preprocess_data(row['text']), 'label': row['generated']}, index=[0])], ignore_index=True)\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    test_tokenized_df = pd.concat([test_tokenized_df, pd.DataFrame({'tokens': preprocess_data(row['text']), 'label': row['label']}, index=[0])], ignore_index=True)\n",
    "\n",
    "train_data, pretest_data = train_test_split(train_pretest_tokenized_df, stratify=train_pretest_tokenized_df.label, test_size=0.2)\n",
    "\n",
    "train_data.to_csv(\"data/baseline_processed_train_data.csv\")\n",
    "pretest_data.to_csv(\"data/baseline_processed_pretest_data.csv\")\n",
    "test_tokenized_df.to_csv(\"data/baseline_processed_test_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the training data: 126635\n",
      "Number of entries in the test data: 31659\n",
      "\n",
      "\n",
      "Proportion of the data:\n",
      "\n",
      "                     Data overall         Train data           Test data           \n",
      "Human written        0.2288               0.2288               0.2288              \n",
      "LLM generated        0.7712               0.7712               0.7712              \n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"data/baseline_processed_train_data.csv\")\n",
    "pretest_data = pd.read_csv(\"data/baseline_processed_pretest_data.csv\")\n",
    "test_data = pd.read_csv(\"data/baseline_processed_test_data.csv\")\n",
    "\n",
    "print(f\"Number of entries in the training data: {train_data.shape[0]}\")\n",
    "print(f\"Number of entries in the test data: {pretest_data.shape[0]}\")\n",
    "\n",
    "data_label_0_proportion = train_and_pretest_data[train_and_pretest_data.generated == 0].shape[0] / train_and_pretest_data.shape[0]\n",
    "data_label_1_proportion = train_and_pretest_data[train_and_pretest_data.generated == 1].shape[0] / train_and_pretest_data.shape[0]\n",
    "\n",
    "train_data_label_0_propotion = train_data[train_data.label == 0].shape[0] / train_data.shape[0]\n",
    "train_data_label_1_propotion = train_data[train_data.label == 1].shape[0] / train_data.shape[0]\n",
    "\n",
    "pretest_data_label_0_propotion = pretest_data[pretest_data.label == 0].shape[0] / pretest_data.shape[0]\n",
    "pretest_data_label_1_propotion = pretest_data[pretest_data.label == 1].shape[0] / pretest_data.shape[0]\n",
    "\n",
    "print(\"\\n\\nProportion of the data:\")\n",
    "print(f\"\\n{'':<20s} {'Data overall':<20s} {'Train data':<20s} {'Test data':<20s}\")\n",
    "print(f\"{'Human written':<20s} {data_label_0_proportion:<20.4f} {train_data_label_0_propotion:<20.4f} {pretest_data_label_0_propotion:<20.4f}\")\n",
    "print(f\"{'LLM generated':<20s} {data_label_1_proportion:<20.4f} {train_data_label_1_propotion:<20.4f} {pretest_data_label_1_propotion:<20.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the features of the training data: 126635\n",
      "Number of entries in the features of the test data: 31659\n",
      "({'effects': True, 'of': True, 'adding': True, 'an': True, 'additional': True, '1.5': True, 'hours': True, 'to': True, 'the': True, 'school': True, 'day': True, 'on': True, 'student': True, 'learning': True, 'and': True, 'grades': True, '.': True, 'it': True, 'is': True, 'widely': True, 'accepted': True, 'that': True, 'students': True, 'need': True, 'more': True, 'time': True, 'learn': True, 'succeed': True, 'academically': True, 'in': True, 'recent': True, 'years': True, ',': True, 'there': True, 'has': True, 'been': True, 'a': True, 'growing': True, 'trend': True, 'towards': True, 'extra': True, 'with': True, 'some': True, 'schools': True, 'even': True, 'extending': True, 'by': True, 'up': True, 'purpose': True, 'this': True, 'essay': True, 'explore': True, 'evidence': True, 'supporting': True, 'idea': True, 'providing': True, 'for': True, 'helps': True, 'examine': True, 'personal': True, 'anecdote': True, 'illustrating': True, 'how': True, 'performance': True, 'affected': True, 'when': True, 'not': True, 'enough': True, 'complete': True, 'their': True, 'classwork': True, 'one': True, 'most': True, 'significant': True, 'pieces': True, 'study': True, 'conducted': True, 'rand': True, 'corporation': True, '2011.': True, 'found': True, '90': True, 'minutes': True, 'led': True, 'improvement': True, 'achievement': True, 'particularly': True, 'low-income': True, 'also': True, 'benefits': True, 'were': True, 'pronounced': True, 'subjects': True, 'such': True, 'as': True, 'math': True, 'science': True, 'where': True, 'able': True, 'spend': True, 'practicing': True, 'applying': True, 'new': True, 'concepts': True, 'another': True, 'piece': True, 'success': True, 'educational': True, 'systems': True, 'around': True, 'world': True, 'incorporate': True, 'longer': True, 'days': True, 'example': True, 'finland': True, 'attend': True, 'only': True, '6': True, 'but': True, 'they': True, 'consistently': True, 'rank': True, 'among': True, 'top': True, 'performers': True, 'international': True, 'tests': True, 'attributed': True, 'fact': True, 'finnish': True, 'have': True, 'access': True, 'wide': True, 'range': True, 'extracurricular': True, 'activities': True, 'opportunities': True, 'hands-on': True, 'which': True, 'develop': True, 'critical': True, 'thinking': True, 'problem-solving': True, 'skills': True, 'my': True, 'own': True, 'experience': True, 'challenging': True, 'class': True, 'teacher': True, 'would': True, 'often': True, 'assign': True, 'complex': True, 'problems': True, 'required': True, 'several': True, 'working': True, 'them': True, 'outside': True, 'however': True, 'many': True, 'including': True, 'myself': True, 'did': True, 'or': True, 'resources': True, 'these': True, 'assignments': True, 'best': True, 'ability': True, 'result': True, 'i': True, 'struggling': True, 'keep': True, 'course': True, 'material': True, 'began': True, 'suffer': True, 'conclusion': True, 'compelling': True}, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_features(text):\n",
    "    \"\"\"\n",
    "    A simple feature extractor, based on Kochmar, 2022, p. 171\n",
    "\n",
    "    :param text: a string\n",
    "    :return: a dictionary of features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        features[word.lower()] = True\n",
    "    return features\n",
    "\n",
    "train_features = [(get_features(row['tokens']), row['label']) for _, row in train_data.iterrows()]\n",
    "pretest_features = [(get_features(row['tokens']), row['label']) for _, row in pretest_data.iterrows()]\n",
    "test_features = [(get_features(row['tokens']), row['label']) for _, row in test_data.iterrows()]\n",
    "\n",
    "print(f\"Number of entries in the features of the training data: {len(train_features)}\")\n",
    "print(f\"Number of entries in the features of the test data: {len(pretest_features)}\")\n",
    "\n",
    "print(train_features[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the training data: 0.7442137690024908\n",
      "F1 score on the pretest data: 0.7263287885279367\n",
      "F1 score on the test data: 0.9605975154332749\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "labels_train = [label for _, label in train_features]\n",
    "labels_pretest = [label for _, label in pretest_features]\n",
    "labels_test = [label for _, label in test_features]\n",
    "predicted_labels_train = [classifier.classify(featureset) for featureset, _ in train_features]\n",
    "predicted_labels_pretest = [classifier.classify(featureset) for featureset, _ in pretest_features]\n",
    "predicted_labels_test = [classifier.classify(featureset) for featureset, _ in test_features]\n",
    "print(f\"F1 score on the training data: {f1_score(labels_train, predicted_labels_train)}\")\n",
    "print(f\"F1 score on the pretest data: {f1_score(labels_pretest, predicted_labels_pretest)}\")\n",
    "print(f\"F1 score on the test data: {f1_score(labels_test, predicted_labels_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9w/2pytp4_s6bj5_ff4zz3vl8ph0000gn/T/ipykernel_16176/592256311.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_v2_tokenized_df = pd.concat([test_v2_tokenized_df, pd.DataFrame({'tokens': preprocess_data(row['text']), 'label': row['generated']}, index=[0])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the test data v2: 0.8521676368254458\n"
     ]
    }
   ],
   "source": [
    "test_v2_data = pd.read_csv('data/ai_vs_human.csv')\n",
    "train_and_pretest_data = pd.read_csv('data/detect_ai.csv')\n",
    "test_data = pd.read_csv('data/daigt_v4.csv')\n",
    "\n",
    "test_v2_data = test_v2_data[~test_v2_data['text'].isin(train_and_pretest_data['text'])]\n",
    "test_v2_data = test_v2_data[~test_v2_data['text'].isin(test_data['text'])]\n",
    "\n",
    "print(len(test_v2_data))\n",
    "\n",
    "test_v2_tokenized_df = pd.DataFrame(columns=['tokens', 'label'])\n",
    "\n",
    "for index, row in test_v2_data.iterrows():\n",
    "    test_v2_tokenized_df = pd.concat([test_v2_tokenized_df, pd.DataFrame({'tokens': preprocess_data(row['text']), 'label': row['generated']}, index=[0])], ignore_index=True)\n",
    "\n",
    "test_v2_features = [(get_features(row['tokens']), row['label']) for _, row in test_v2_tokenized_df.iterrows()]\n",
    "\n",
    "labels_test_v2 = [label for _, label in test_v2_features]\n",
    "predicted_labels_test_v2 = [classifier.classify(featureset) for featureset, _ in test_v2_features]\n",
    "print(f\"F1 score on the test data v2: {f1_score(labels_test_v2, predicted_labels_test_v2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
