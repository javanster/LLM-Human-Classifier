{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size before removing rows identical to the training data: 73573\n",
      "Train and pretest data size: 158294\n",
      "Test data size: 40202\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_and_pretest_data = pd.read_csv('data/detect_ai.csv')\n",
    "test_data = pd.read_csv('data/daigt_v4.csv')\n",
    "\n",
    "print(f\"Test data size before removing rows identical to the training data: {len(test_data)}\")\n",
    "\n",
    "train_and_pretest_data = train_and_pretest_data.drop_duplicates(subset='text')\n",
    "test_data = test_data[~test_data['text'].isin(train_and_pretest_data['text'])]\n",
    "\n",
    "print(f\"Train and pretest data size: {len(train_and_pretest_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "# nltk.download('punkt') # Uncomment this line if you haven't downloaded the 'punkt' package\n",
    "\n",
    "def preprocess_data(data):\n",
    "    tokens = word_tokenize(data.lower())\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "train_pretest_tokenized_df = pd.DataFrame(columns=['tokens', 'label'])\n",
    "test_tokenized_df = pd.DataFrame(columns=['tokens', 'label'])\n",
    "\n",
    "for index, row in train_and_pretest_data.iterrows():\n",
    "    train_pretest_tokenized_df = pd.concat([train_pretest_tokenized_df, pd.DataFrame({'tokens': preprocess_data(row['text']), 'label': row['generated']}, index=[0])], ignore_index=True)\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    test_tokenized_df = pd.concat([test_tokenized_df, pd.DataFrame({'tokens': preprocess_data(row['text']), 'label': row['label']}, index=[0])], ignore_index=True)\n",
    "\n",
    "train_data, pretest_data = train_test_split(train_pretest_tokenized_df, stratify=train_pretest_tokenized_df.label, test_size=0.2)\n",
    "\n",
    "train_data.to_csv(\"data/baseline_processed_train_data.csv\")\n",
    "pretest_data.to_csv(\"data/baseline_processed_pretest_data.csv\")\n",
    "test_tokenized_df.to_csv(\"data/baseline_processed_test_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the training data: 126635\n",
      "Number of entries in the test data: 31659\n",
      "\n",
      "\n",
      "Proportion of the data:\n",
      "\n",
      "                     Data overall         Train data           Test data           \n",
      "Human written        0.2288               0.2288               0.2288              \n",
      "LLM generated        0.7712               0.7712               0.7712              \n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"data/baseline_processed_train_data.csv\")\n",
    "pretest_data = pd.read_csv(\"data/baseline_processed_pretest_data.csv\")\n",
    "test_data = pd.read_csv(\"data/baseline_processed_test_data.csv\")\n",
    "\n",
    "print(f\"Number of entries in the training data: {train_data.shape[0]}\")\n",
    "print(f\"Number of entries in the test data: {pretest_data.shape[0]}\")\n",
    "\n",
    "data_label_0_proportion = train_and_pretest_data[train_and_pretest_data.generated == 0].shape[0] / train_and_pretest_data.shape[0]\n",
    "data_label_1_proportion = train_and_pretest_data[train_and_pretest_data.generated == 1].shape[0] / train_and_pretest_data.shape[0]\n",
    "\n",
    "train_data_label_0_propotion = train_data[train_data.label == 0].shape[0] / train_data.shape[0]\n",
    "train_data_label_1_propotion = train_data[train_data.label == 1].shape[0] / train_data.shape[0]\n",
    "\n",
    "pretest_data_label_0_propotion = pretest_data[pretest_data.label == 0].shape[0] / pretest_data.shape[0]\n",
    "pretest_data_label_1_propotion = pretest_data[pretest_data.label == 1].shape[0] / pretest_data.shape[0]\n",
    "\n",
    "print(\"\\n\\nProportion of the data:\")\n",
    "print(f\"\\n{'':<20s} {'Data overall':<20s} {'Train data':<20s} {'Test data':<20s}\")\n",
    "print(f\"{'Human written':<20s} {data_label_0_proportion:<20.4f} {train_data_label_0_propotion:<20.4f} {pretest_data_label_0_propotion:<20.4f}\")\n",
    "print(f\"{'LLM generated':<20s} {data_label_1_proportion:<20.4f} {train_data_label_1_propotion:<20.4f} {pretest_data_label_1_propotion:<20.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the features of the training data: 126635\n",
      "Number of entries in the features of the test data: 31659\n",
      "({'do': True, 'i': True, 'agree': True, 'with': True, 'what': True, 'the': True, 'principle': True, 'is': True, 'saying': True, '?': True, 'yes': True, 'he': True, 'or': True, 'she': True, 'must': True, 'at': True, 'least': True, 'one': True, 'extracurricular': True, 'activity': True, ',': True, 'another': True, 'reason': True, 'why': True, 'disagree': True, 'because': True, 'if': True, 'a': True, 'kid': True, 'really': True, 'interested': True, 'in': True, 'doing': True, 'something': True, 'that': True, 'good': True, 'to': True, 'them': True, 'then': True, 'they': True, 'should': True, 'be': True, 'able': True, 'decide': True, 'whether': True, 'its': True, 'actually': True, 'want': True, '.': True, 'now': True, 'this': True, 'pro': True, 'con': True, 'sustain': True, 'not': True, 'every': True, 'gon': True, 'na': True, 'commit': True, 'going': True, 'so': True, 'personally': True, 'idea': True, 'has': True, 'time': True, 'and': True, \"n't\": True, 'attend': True, 'it': True, \"'s\": True, 'of': True, 'reasons': True, 'pushed': True, 'on': True, 'him': True, 'her': True, 'an': True, 'extra': True, 'help': True, 'kids': True, 'their': True, 'grades': True, 'other': True, 'things': True, 'can': True, 'you': True, 'feel': True, 'about': True, 'yourself': True, 'everyone': True, 'different': True, 'will': True, 'probably': True, 'get': True, 'common': True, 'thats': True, 'great': True, 'curricular': True, 'but': True, 'there': True, 'are': True, 'some': True, 'dont': True, 'participate': True, 'right': True, 'thing': True, 'also': True, 'think': True, 'pick': True, 'had': True, 'would': True, 'have': True, 'push': True, 'up': True, 'after': True, 'older': True, 'than': True, 'me': True, 'main': True, 'especially': True, 'wasnt': True, 'for': True, 'all': True, 'like': True, 'forcing': True, 'isnt': True, 'bad': True, 'choose': True, 'chose': True, 'wanted': True, 'wouldnt': True, 'go': True, 'activities': True, 'done': True, 'theres': True, 'pros': True, 'cons': True, 'my': True, 'opinion': True, 'school': True, 'could': True, 'just': True, 'everybody': True, 'believe': True, 'lot': True, 'students': True, 'try': True, 'depends': True, 'community': True, 'opino': True, 'opinions': True, 'take': True, 'look': True, 'see': True, 'enjoy': True}, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_features(text):\n",
    "    \"\"\"\n",
    "    A simple feature extractor, based on Kochmar, 2022, p. 171\n",
    "\n",
    "    :param text: a string\n",
    "    :return: a dictionary of features\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    words = text.split(' ')\n",
    "    for word in words:\n",
    "        features[word.lower()] = True\n",
    "    return features\n",
    "\n",
    "train_features = [(get_features(row['tokens']), row['label']) for _, row in train_data.iterrows()]\n",
    "pretest_features = [(get_features(row['tokens']), row['label']) for _, row in pretest_data.iterrows()]\n",
    "test_features = [(get_features(row['tokens']), row['label']) for _, row in test_data.iterrows()]\n",
    "\n",
    "print(f\"Number of entries in the features of the training data: {len(train_features)}\")\n",
    "print(f\"Number of entries in the features of the test data: {len(pretest_features)}\")\n",
    "\n",
    "print(train_features[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the training data: 0.7562833536838582\n",
      "F1 score on the pretest data: 0.7385036042754164\n",
      "F1 score on the test data: 0.9587435270016502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "labels_train = [label for _, label in train_features]\n",
    "labels_pretest = [label for _, label in pretest_features]\n",
    "labels_test = [label for _, label in test_features]\n",
    "predicted_labels_train = [classifier.classify(featureset) for featureset, _ in train_features]\n",
    "predicted_labels_pretest = [classifier.classify(featureset) for featureset, _ in pretest_features]\n",
    "predicted_labels_test = [classifier.classify(featureset) for featureset, _ in test_features]\n",
    "print(f\"F1 score on the training data: {f1_score(labels_train, predicted_labels_train)}\")\n",
    "print(f\"F1 score on the pretest data: {f1_score(labels_pretest, predicted_labels_pretest)}\")\n",
    "print(f\"F1 score on the test data: {f1_score(labels_test, predicted_labels_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxgu\\AppData\\Local\\Temp\\ipykernel_4256\\592256311.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_v2_tokenized_df = pd.concat([test_v2_tokenized_df, pd.DataFrame({'tokens': preprocess_data(row['text']), 'label': row['generated']}, index=[0])], ignore_index=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m test_v2_tokenized_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m test_v2_data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m---> 13\u001b[0m     test_v2_tokenized_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_v2_tokenized_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerated\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m test_v2_features \u001b[38;5;241m=\u001b[39m [(get_features(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]), row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m test_v2_tokenized_df\u001b[38;5;241m.\u001b[39miterrows()]\n\u001b[0;32m     17\u001b[0m labels_test_v2 \u001b[38;5;241m=\u001b[39m [label \u001b[38;5;28;01mfor\u001b[39;00m _, label \u001b[38;5;129;01min\u001b[39;00m test_v2_features]\n",
      "File \u001b[1;32mc:\\Projects\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Projects\\.venv\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Projects\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_v2_data = pd.read_csv('data/ai_vs_human.csv')\n",
    "train_and_pretest_data = pd.read_csv('data/detect_ai.csv')\n",
    "test_data = pd.read_csv('data/daigt_v4.csv')\n",
    "\n",
    "test_v2_data = test_v2_data[~test_v2_data['text'].isin(train_and_pretest_data['text'])]\n",
    "test_v2_data = test_v2_data[~test_v2_data['text'].isin(test_data['text'])]\n",
    "\n",
    "print(len(test_v2_data))\n",
    "\n",
    "test_v2_tokenized_df = pd.DataFrame(columns=['tokens', 'label'])\n",
    "\n",
    "for index, row in test_v2_data.iterrows():\n",
    "    test_v2_tokenized_df = pd.concat([test_v2_tokenized_df, pd.DataFrame({'tokens': preprocess_data(row['text']), 'label': row['generated']}, index=[0])], ignore_index=True)\n",
    "\n",
    "test_v2_features = [(get_features(row['tokens']), row['label']) for _, row in test_v2_tokenized_df.iterrows()]\n",
    "\n",
    "labels_test_v2 = [label for _, label in test_v2_features]\n",
    "predicted_labels_test_v2 = [classifier.classify(featureset) for featureset, _ in test_v2_features]\n",
    "print(f\"F1 score on the test data v2: {f1_score(labels_test_v2, predicted_labels_test_v2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
